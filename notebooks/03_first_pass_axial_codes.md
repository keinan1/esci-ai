# Axial Coding

## Ministral-3:3b

I first performed an error analysis by manually going through the traces for the false positives and false negatives, making observations and suggesting possible fixes. See `first_pass_error_analysis.md`.

I then put my error analysis into Claude, to generate Axial Codes. These are from Claude:

### AXIAL CODES AND PROMPT IMPROVEMENTS GENERATED BY CLAUDE

### Category 1: Reasoning-before-answer ordering

| Error | Description |
|-------|-------------|
| FP1   | The model commits to a classification first, then reasons itself into the opposite conclusion too late. The structured output forces a premature commitment. |

### Category 2: Overly literal matching

| Error | Description |
|-------|-------------|
| FN3   | "100-count" vs "100 pack" — treats packaging phrasing as a specification mismatch |
| FN4   | Battery-powered tool not recognized as cordless without explicit "cordless" keyword |
| FN5   | Product title contains "Kit" but model demands additional accessory enumeration |
| FN6   | "8-1/2 x 11" treated as different from "8.5 x 11" |

The model demands exact textual/lexical match rather than semantic equivalence, treating minor phrasing differences as specification failures.

### Category 3: Subset misunderstanding

| Error | Description |
|-------|-------------|
| FN1   | Model penalizes the product for having *more* features than the query specifies. Treats "exact match" as requiring bidirectional equivalence rather than the query specifications being a subset of the product attributes. |

### Category 4: Conflicting product information

| Error | Description |
|-------|-------------|
| FN2   | Title says 100-count, bullet says 50-count. The model doesn't know how to reconcile contradictory signals. This is genuinely ambiguous and arguably correct behavior. |

---

## Prompt Improvements

Two changes address categories 1–3: reorder fields in the Pydantic model (forces chain-of-thought before commitment), and sharpen the system prompt.

FN2 (conflicting product data) is left unaddressed since it's genuinely ambiguous and the model's behavior is arguably reasonable.

### 1. Pydantic Model: Swap Field Order

Placing `reasoning` before `match_classification` forces the model to think before committing to a label. The `reasoning` field description is also tightened to enforce structured enumeration of query specs.

```python
class QueryProductMatch(BaseModel):
    reasoning: str = Field(
        ...,
        description=(
            "Succinct reasoning. First list each explicit query specification. "
            "Then for each, state whether the product satisfies it. "
            "If all are satisfied, state 'All query specifications satisfied.' "
            "Otherwise, cite precisely which specification(s) are not met."
        ),
    )
    match_classification: MatchClassification = Field(
        ...,
        description="Classification based on the reasoning above.",
    )
```

**Fixes:** Category 1 (reasoning-before-answer ordering)

### 2. System Prompt: Add Match Definition & Common-Sense Guidance

```python
classifier_system_prompt = """
<TASK OVERVIEW>
You are a world-class quality assurance agent responsible for improving a product query system.
Your task is to examine a query-product pair and determine whether the product is an exact match
for the query.
</TASK OVERVIEW>

<DEFINITION OF EXACT MATCH>
A product is an exact match when every specification in the query is satisfied by the product
information.
- The product MAY have additional features, details, or attributes beyond what the query asks
  for. This is fine — only the query's specifications must be met.
- Match on MEANING, not exact wording.
- Use common sense and domain knowledge to interpret both queries and product details.
</DEFINITION OF EXACT MATCH>

<INSTRUCTIONS>
1. List every explicit specification in the query.
2. For each specification, check whether the product information satisfies it (by meaning, not
   literal text).
3. Only classify as not_exact_match if a query specification is clearly CONTRADICTED or UNMET
   by the product information.
4. Output your reasoning FIRST, then your classification.
</INSTRUCTIONS>
"""
```

**Fixes:**

| Addition | Addresses |
|----------|-----------|
| "Product MAY have additional features" | Category 3 (subset misunderstanding) |
| "Match on MEANING, not exact wording" with examples | Category 2 (overly literal matching) |
| "CONTRADICTED or UNMET" threshold | Category 2 (raises the bar for rejection) |
| "Output reasoning FIRST" | Category 1 (reinforces field-order fix) |

---

## Summary of Changes

| Problem | Root Cause | Fix |
|---------|-----------|-----|
| FP1: Model contradicts itself | Classifies before reasoning | Swap field order; reasoning-first instruction |
| FN1: Extra features penalized | Misunderstands "exact match" as bidirectional | Explicit subset language in prompt |
| FN3, FN5: Phrasing ≠ specs | Lexical vs semantic matching | "Match on meaning" + examples |
| FN4: Cordless not inferred | No common-sense reasoning | "Use common sense and domain knowledge" |
| FN6: Notation difference | Literal text comparison | Concrete example in prompt ("8.5 x 11" = "8-1/2 x 11") |
| FN2: Conflicting product data | Genuinely ambiguous | No change (arguably correct) |
